{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acfc06bb",
   "metadata": {},
   "source": [
    "# Fake News Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62873b3e",
   "metadata": {},
   "source": [
    "### Generic Purpose\n",
    "\n",
    "The dataset is meant to support fake news detection. In other words, it provides labeled examples of news articles (real vs. fake) that can be used to train, test, and evaluate machine learning or NLP models. The goal is to help systems automatically distinguish misinformation from reliable reporting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188aa8c",
   "metadata": {},
   "source": [
    "#### Two Specific Objectives\n",
    "\n",
    "##### Build and Evaluate a Classifier\n",
    "<ul>\n",
    "    <li>Use the title and/or text fields to train a supervised learning model that predicts the label (fake vs. real).</li>\n",
    "    <li>Benchmark different models (e.g., logistic regression, random forest, transformers) to see which performs best.</li>\n",
    "</ul>\n",
    "\n",
    "##### Analyze Patterns of Fake vs. Real News\n",
    "<ul>\n",
    "<li>Explore linguistic and metadata differences (e.g., vocabulary, sentiment, writing style, publishing source, frequency of certain words).</li>\n",
    "<li>Identify features that strongly correlate with fake news, which could also help in understanding misinformation strategies.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f016520",
   "metadata": {},
   "source": [
    "First things first, we are installing the following:\n",
    "\n",
    "#### Pandas\n",
    "\n",
    "A toolkit for organizing data into something we can work with.\n",
    "Given that we have a CSV with thousands of articles, each with columns like <i>title, content, source, and label(fake/real)</i>.\n",
    "Pandas gives us DataFrames, which are like spreadsheets in Python.\n",
    "\n",
    "This enables us to: \n",
    "<ul>\n",
    "    <li>Drop duplicates or empty rows (some articles might be missing content).</li>\n",
    "    <li>Normalize text (e.g., lowercasing, removing weird spacing).</li>\n",
    "    <li>Filter out specific sources or select only certain date ranges.</li>\n",
    "    <li>Join/merge multiple datasets if you‚Äôre combining sources.</li>\n",
    "</ul>\n",
    "\n",
    "#### NumPy\n",
    "\n",
    "Lives underneath Pandas and makes all the number crunching fast. \n",
    "Even though text data feels ‚Äúnon-numeric,‚Äù eventually you turn it into numbers (word counts, TF-IDF values, embeddings). \n",
    "\n",
    "NumPy handles:\n",
    "<ul>\n",
    "    <li>Efficient arrays (much faster than vanilla Python lists).</li>\n",
    "    <li>Math operations on big chunks of data at once, instead of looping line by line.</li>\n",
    "    <li>The foundations of most machine learning libraries ‚Äî scikit-learn, TensorFlow, PyTorch all lean on NumPy arrays behind the scenes.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03e3d03",
   "metadata": {},
   "source": [
    "##### Pandas üêº for the data cleaning layer.\n",
    "##### NumPy üî¢ is the engine under the hood that makes it all run efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00701f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230f06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./fake_news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e4078b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   title     20000 non-null  object\n",
      " 1   text      20000 non-null  object\n",
      " 2   date      20000 non-null  object\n",
      " 3   source    19000 non-null  object\n",
      " 4   author    19000 non-null  object\n",
      " 5   category  20000 non-null  object\n",
      " 6   label     20000 non-null  object\n",
      "dtypes: object(7)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8d7771",
   "metadata": {},
   "source": [
    "#### Number of Rows & Columns\n",
    "\n",
    "There are 20,000 rows (entries).\n",
    "\n",
    "There are 7 columns in total.\n",
    "\n",
    "#### Columns Overview\n",
    "\n",
    "title: headline of the article\n",
    "\n",
    "text: main body of the article\n",
    "\n",
    "date: publication date\n",
    "\n",
    "source: news source/publisher\n",
    "\n",
    "author: author name (some missing values: 19,000 non-null ‚Üí ~1,000 missing)\n",
    "\n",
    "category: probably the section of the article (politics, tech, etc.)\n",
    "\n",
    "label: this is most likely the target (fake or real news indicator)\n",
    "\n",
    "#### Data Types\n",
    "\n",
    "All columns are currently object type ‚Äî so text-based (strings).\n",
    "\n",
    "Even date is an object right now, not yet converted to a datetime type.\n",
    "\n",
    "#### Memory Usage\n",
    "\n",
    "About 1.1 MB, which is tiny ‚Äî so the dataset is lightweight and easy to work with.\n",
    "\n",
    "#### Data Quality Check\n",
    "\n",
    "No missing values in most columns except author.\n",
    "\n",
    "If you‚Äôre doing text analysis, the main columns of interest will be title, text, and label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5dedc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Foreign Democrat final.</td>\n",
       "      <td>more tax development both store agreement lawy...</td>\n",
       "      <td>2023-03-10</td>\n",
       "      <td>NY Times</td>\n",
       "      <td>Paula George</td>\n",
       "      <td>Politics</td>\n",
       "      <td>real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>To offer down resource great point.</td>\n",
       "      <td>probably guess western behind likely next inve...</td>\n",
       "      <td>2022-05-25</td>\n",
       "      <td>Fox News</td>\n",
       "      <td>Joseph Hill</td>\n",
       "      <td>Politics</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Himself church myself carry.</td>\n",
       "      <td>them identify forward present success risk sev...</td>\n",
       "      <td>2022-09-01</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Julia Robinson</td>\n",
       "      <td>Business</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You unit its should.</td>\n",
       "      <td>phone which item yard Republican safe where po...</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Mr. David Foster DDS</td>\n",
       "      <td>Science</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Billion believe employee summer how.</td>\n",
       "      <td>wonder myself fact difficult course forget exa...</td>\n",
       "      <td>2023-04-03</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Austin Walker</td>\n",
       "      <td>Technology</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  title  \\\n",
       "0               Foreign Democrat final.   \n",
       "1   To offer down resource great point.   \n",
       "2          Himself church myself carry.   \n",
       "3                  You unit its should.   \n",
       "4  Billion believe employee summer how.   \n",
       "\n",
       "                                                text        date    source  \\\n",
       "0  more tax development both store agreement lawy...  2023-03-10  NY Times   \n",
       "1  probably guess western behind likely next inve...  2022-05-25  Fox News   \n",
       "2  them identify forward present success risk sev...  2022-09-01       CNN   \n",
       "3  phone which item yard Republican safe where po...  2023-02-07   Reuters   \n",
       "4  wonder myself fact difficult course forget exa...  2023-04-03       CNN   \n",
       "\n",
       "                 author    category label  \n",
       "0          Paula George    Politics  real  \n",
       "1           Joseph Hill    Politics  fake  \n",
       "2        Julia Robinson    Business  fake  \n",
       "3  Mr. David Foster DDS     Science  fake  \n",
       "4         Austin Walker  Technology  fake  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02adcee",
   "metadata": {},
   "source": [
    "the ```data.head()``` shows us the first 5 rows of our DataFrame, so we can peek at what the dataset actually looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6c566",
   "metadata": {},
   "source": [
    "*   **Structure & Sample Records**\n",
    "    \n",
    "    *   Each row is one **news article**.\n",
    "        \n",
    "    *   Columns match what we saw before: title, text, date, source, author, category, and label.\n",
    "        \n",
    "*   **Examples of Content**\n",
    "    \n",
    "    *   Titles are short headlines like _‚ÄúForeign Democrat final.‚Äù_ or _‚ÄúTo offer down resource great point.‚Äù_\n",
    "        \n",
    "    *   text contains longer article bodies (truncated in the preview).\n",
    "        \n",
    "    *   date is a proper timestamp string (like 2023-03-10).\n",
    "        \n",
    "    *   source names publishers (NY Times, Fox News, CNN, Reuters).\n",
    "        \n",
    "    *   author column has individual names.\n",
    "        \n",
    "    *   category shows the article section (Politics, Business, Science, Technology).\n",
    "        \n",
    "    *   label is the target (either **real** or **fake**).\n",
    "        \n",
    "*   **Quick Observations**\n",
    "    \n",
    "    *   The dataset is **balanced in terms of fake/real** at least in this small preview (2 ‚Äúreal‚Äù, 3 ‚Äúfake‚Äù).\n",
    "        \n",
    "    *   Text looks a little odd / randomly generated (‚ÄúHimself church myself carry...‚Äù), which suggests this might be a **synthetic dataset** rather than real journalism.\n",
    "        \n",
    "    *   Dates span across different years, so temporal patterns could also be analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb30a3",
   "metadata": {},
   "source": [
    "### De-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7778a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset=[\"title\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce5c7e",
   "metadata": {},
   "source": [
    "üëâ Ensures that the dataset doesn‚Äôt have multiple rows with the exact same article (title + text). Prevents model bias from duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2dd2e",
   "metadata": {},
   "source": [
    "### Missing-value normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab804c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.replace(r\"^\\s*$\", np.nan, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4a3654",
   "metadata": {},
   "source": [
    "üëâ Converts blank or whitespace-only strings into NaN, making missing values easier to detect and handle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f72a426",
   "metadata": {},
   "source": [
    "### Type casting for dates + calendar parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e611b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'], errors='coerce')\n",
    "\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3662f",
   "metadata": {},
   "source": [
    "üëâ Converts date into a proper datetime object. If conversion fails, it sets them as NaT (Not a Time).\n",
    "<br>\n",
    "<br>\n",
    "üëâ Then extracts year, month, and day ‚Üí useful for time-based analysis or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e622932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = data['date'].fillna(pd.NaT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb01d617",
   "metadata": {},
   "source": [
    "üëâ Ensures missing dates are explicitly marked as NaT rather than leaving them undefined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080f62e",
   "metadata": {},
   "source": [
    "### String cleanup for object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bbdf14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = data.select_dtypes(include=\"object\").columns\n",
    "for col in object_cols:\n",
    "    data[col] = data[col].fillna('unknown')\n",
    "    data[col] = data[col].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0623d850",
   "metadata": {},
   "source": [
    "üëâ For all categorical/text columns:\n",
    "    \n",
    "*   Fills missing values with \"unknown\".\n",
    "    \n",
    "*   Strips leading/trailing whitespace and standardizes everything to lowercase ‚Üí prevents issues like \"CNN\", \"cnn \", \"cnn\" being treated differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb164fdd",
   "metadata": {},
   "source": [
    "### Source normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a5ef2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_mapping = {\n",
    "    \"bbc news\": \"bbc\",\n",
    "    \"bbc.com\": \"bbc\",\n",
    "    \"bbc.co.uk\": \"bbc\",\n",
    "    \"bbc\": \"bbc\",\n",
    "\n",
    "    \"cnn news\": \"cnn\",\n",
    "    \"cnn.com\": \"cnn\",\n",
    "    \"cnn\": \"cnn\",\n",
    "\n",
    "    \"foxnews.com\": \"fox news\",\n",
    "    \"fox news\": \"fox news\",\n",
    "    \"fox\": \"fox news\",\n",
    "\n",
    "    \"ny times\": \"new york times\",\n",
    "    \"nytimes.com\": \"new york times\",\n",
    "    \"new york times\": \"new york times\",\n",
    "    \"nyt\": \"new york times\",\n",
    "\n",
    "    \"reuters.com\": \"reuters\",\n",
    "    \"reuters\": \"reuters\",\n",
    "\n",
    "    \"dailynews.com\": \"daily news\",\n",
    "    \"daily news\": \"daily news\",\n",
    "\n",
    "    \"global times\": \"global times\",\n",
    "    \"globaltimes.cn\": \"global times\",\n",
    "\n",
    "    \"guardian.co.uk\": \"the guardian\",\n",
    "    \"the guardian\": \"the guardian\",\n",
    "    \"guardian\": \"the guardian\",\n",
    "\n",
    "    \"unknown\": \"unknown\",\n",
    "    \"\": \"unknown\",\n",
    "    \"n/a\": \"unknown\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7522ca",
   "metadata": {},
   "source": [
    "üëâ Standardizes publisher names into consistent categories. For example:\n",
    "\n",
    "* \"bbc.com\", \"bbc news\", \"bbc\" ‚Üí all mapped to \"bbc\".\n",
    "\n",
    "* \"cnn.com\", \"cnn\" ‚Üí all mapped to \"cnn\".\n",
    "\n",
    "* Missing or odd entries like \"n\" or \"n/a\" ‚Üí mapped to \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ad5a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"source\"] = data[\"source\"].str.lower().map(source_mapping).fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13bbfa6",
   "metadata": {},
   "source": [
    "üëâ Cleans the source column:\n",
    "\n",
    "* Lowercases everything.\n",
    "\n",
    "* Maps raw source names to the standardized values defined in source_mapping.\n",
    "\n",
    "* Any unmapped source becomes \"unknown\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431b409",
   "metadata": {},
   "source": [
    "### Feature engineering (text length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ced3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"text_length\"] = data[\"text\"].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20a56f6",
   "metadata": {},
   "source": [
    "üëâ Creates a new column text_length that stores the number of characters in each article‚Äôs text.\n",
    "* This is useful for filtering out junk rows (like very short texts) and possibly as a feature in classification (fake news might systematically differ in length)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d2bcc",
   "metadata": {},
   "source": [
    "### Content sanity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "addf92e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[(data[\"text_length\"] > 30) & (data[\"text_length\"] < 10000)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8935cea",
   "metadata": {},
   "source": [
    "üëâ Keeps only articles with length between 30 and 10,000 characters.\n",
    "\n",
    "* Removes noise: texts that are too short (like one-liners) or suspiciously long (maybe corrupted).\n",
    "\n",
    "* Ensures the dataset has meaningful article content for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8933fa",
   "metadata": {},
   "source": [
    "### Final completeness check (QA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16e9058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values:\n",
      " title          0\n",
      "text           0\n",
      "date           0\n",
      "source         0\n",
      "author         0\n",
      "category       0\n",
      "label          0\n",
      "year           0\n",
      "month          0\n",
      "day            0\n",
      "text_length    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Remaining missing values:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a98459",
   "metadata": {},
   "source": [
    "üëâ Verifies whether any columns still have missing values after cleaning.\n",
    "\n",
    "* Output shows 0 missing values across all columns ‚Üí dataset is now fully clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "64e63ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Authors (sample): ['paula george' 'joseph hill' 'julia robinson' 'mr. david foster dds'\n",
      " 'austin walker' 'sherri fry' 'alyssa young' 'tina garrett'\n",
      " 'heather greene' 'erin hanson']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique Authors (sample):\", data[\"author\"].unique()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089779f",
   "metadata": {},
   "source": [
    "üëâ Prints the first 10 unique authors to check if author cleaning worked.\n",
    "\n",
    "* Sample shows names like \"paula george\", \"joseph hill\", \"mr. david foster dds\".\n",
    "\n",
    "* This confirms that author names are standardized (lowercase, no blanks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a74b02e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Sources (sample): ['new york times' 'fox news' 'cnn' 'reuters' 'daily news' 'global times'\n",
      " 'the guardian' 'bbc' 'unknown']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nUnique Sources (sample):\", data[\"source\"].unique()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98e4b07",
   "metadata": {},
   "source": [
    "üëâ Prints the first 10 unique news sources after normalization.\n",
    "\n",
    "* Sample shows: \"new york times\", \"fox news\", \"cnn\", \"reuters\", \"bbc\", \"unknown\".\n",
    "\n",
    "* Confirms your source_mapping step worked ‚Äî sources are now consistent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8d417ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   title        20000 non-null  object        \n",
      " 1   text         20000 non-null  object        \n",
      " 2   date         20000 non-null  datetime64[ns]\n",
      " 3   source       20000 non-null  object        \n",
      " 4   author       20000 non-null  object        \n",
      " 5   category     20000 non-null  object        \n",
      " 6   label        20000 non-null  object        \n",
      " 7   year         20000 non-null  int32         \n",
      " 8   month        20000 non-null  int32         \n",
      " 9   day          20000 non-null  int32         \n",
      " 10  text_length  20000 non-null  int64         \n",
      "dtypes: datetime64[ns](1), int32(3), int64(1), object(6)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ac890",
   "metadata": {},
   "source": [
    "*   All 20,000 rows are present.\n",
    "    \n",
    "*   No missing values (cleaned nicely üëç).\n",
    "    \n",
    "*   Dates are properly converted into datetime64.\n",
    "    \n",
    "*   New engineered columns (year, month, day, text\\_length) are stored as numeric types.\n",
    "    \n",
    "*   Dataset is small (1.4 MB), so very manageable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253ad69c",
   "metadata": {},
   "source": [
    "### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "989175d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('cleaned_fake_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7de07f",
   "metadata": {},
   "source": [
    "üëâ It will save the file directly into your working directory (the same folder where your script or notebook is running)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
